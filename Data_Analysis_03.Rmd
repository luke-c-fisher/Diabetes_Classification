---
title: "Data Analysis"
author: "Luke Fisher"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::github_document
---

## Introduction

Diabetes is an chronic autoimmune disease affecting millions of Americans each year. It is best described as the body's inability to properly produce insulin, or produce any at all. This is a result of either an invalid or exhausted pancreas, whose job is to secrete enough insulin to manage blood-glucose levels. Normally, insulin is released to enable cells to absorb the blood-glucose to use for energy. In this way, it acts as a "key" between blood-glucose and cells. 

For a diabetic, however, this "key" doesn't occur naturally, instead taking the form of insulin injections. As such, a diabetic uses a glucose monitor to regulate their blood sugar--whose excess or lack thereof has detrimental consequences. For this reason, it is important to know whether or not someone is diabetic. In this project, I will use classification to identify diabetes.   

## Data Collection

The classification will be based on a dataset from the CDCs Behavioral Risk Factor Surveillance System (BRFSS). The data contains 70,692 responses from the 2015 BRFSS survey, each related to risk factors like smoking, high cholesterol, and physical activity. Furthermore, the data contains an equal 50-50 split of respondents with and without diabetes. 

The data is binary, meaning that the predictors take on a value one or zero depending on whether the condition is true or not. For instance, if a respondent has a smoking habit they will be marked with a 1 for the smoking column; otherwise, they will receive a 0. There are some exceptions to this like BMI and age, where the values are continuous. 

## Methodology

The classification will be done by binary logistic regression. As such, the response variable, Diabetes, will take on two values, "yes" or "no", corresponding to whether the patient has the disease. The classification will start with a series of logistic models, each with a unique cutoff. The function will label, respectively, "yes" and "no" for instances of Diabetes above or below a given cutoff. Afterwards, these predicted values will be compared with the actual values in a table and put into a confusion matrix for evaluation. The goal is to isolate and optimize one model for classification. As such, modifications to this model will be taken if the it exhibits unacceptable metrics.  

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(dplyr)
library(ggplot2)
library(ISLR)
library(tibble)
library(caret)
library(tidyr)
library(skimr)
library(glmnet)
library(car)
library(xgboost)
library(pROC)
```


```{r}
diabetesData <- read.csv('/Users/lukefisher/Desktop/Coding/repos/Health_Analytics/Data/Diabetes_Indicators_Binary.csv')
```

## Data Wrangling
```{r}
diabetesData <- diabetesData %>% 
rename(Diabetes = Diabetes_binary) %>%
mutate(Diabetes = factor(Diabetes, levels = c(0, 1), labels = c("no", "yes")))

```


```{r}
# Split the data into an 80/20 train vs. test split. Set the seed for replicability.
set.seed(44222)

diabetesIdx = sample((nrow(diabetesData)), size = 0.8 * nrow(diabetesData))
diabetesTrn = diabetesData[diabetesIdx, ]
diabetesTst = diabetesData[-diabetesIdx, ]

dataHead <- head(diabetesTrn, n = 10)

knitr::kable(dataHead)
```

## Building classifiers

```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}


# Creating separate predictions based on different cutoffs

lrgModel = glm(Diabetes ~ ., data = diabetesTrn, family = "binomial")

testPred_01 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.1)

testPred_02 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.33)

testPred_03 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.5)

testPred_04 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.66)

testPred_05 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.9)


# Evaluate Accuaracy, Sensitivity, and Specificity for each cutoff
testTab_01 <- table(predicted = testPred_01, actual = diabetesTst$Diabetes)
testTab_02 <- table(predicted = testPred_02, actual = diabetesTst$Diabetes)
testTab_03 <- table(predicted = testPred_03, actual = diabetesTst$Diabetes)
testTab_04 <- table(predicted = testPred_04, actual = diabetesTst$Diabetes)
testTab_05 <- table(predicted = testPred_05, actual = diabetesTst$Diabetes)


testMatrx_01 <- confusionMatrix(testTab_01, positive = "yes")
testMatrx_02 <- confusionMatrix(testTab_02, positive = "yes")
testMatrx_03 <- confusionMatrix(testTab_03, positive = "yes")
testMatrx_04 <- confusionMatrix(testTab_04, positive = "yes")
testMatrx_05 <- confusionMatrix(testTab_05, positive = "yes")


metrics <- rbind(
  c(testMatrx_01$overall["Accuracy"],
    testMatrx_01$byClass["Sensitivity"],
    testMatrx_01$byClass["Specificity"]),

  c(testMatrx_02$overall["Accuracy"],
    testMatrx_02$byClass["Sensitivity"],
    testMatrx_02$byClass["Specificity"]),

  c(testMatrx_03$overall["Accuracy"],
    testMatrx_03$byClass["Sensitivity"],
    testMatrx_03$byClass["Specificity"]),
    
  c(testMatrx_04$overall["Accuracy"],
    testMatrx_04$byClass["Sensitivity"],
    testMatrx_04$byClass["Specificity"]),

  c(testMatrx_05$overall["Accuracy"],
    testMatrx_05$byClass["Sensitivity"],
    testMatrx_05$byClass["Specificity"])
)

rownames(metrics) = c("c = 0.10", "c = 0.33", "c = 0.50", "c = 0.66", "c = 0.90")

metrics_tibble <- as_tibble(metrics, rownames = "Threshold")

knitr::kable(metrics_tibble)
```


The table above contains regression models with varying cutoffs. The model with a 0.5 cutoff appears to have the most balanced trade-off between Accuracy, Specificity, and Sensitivity, exhibiting characteristics of a valid classifier. 

## Test errors

```{r}
# Test for error in the 0.5 prediction
calcErr = function(actual, predicted) {
  mean(actual != predicted)
}

logModelErr <- calcErr(actual = diabetesTst$Diabetes, predicted = testPred_03)

knitr::kable(logModelErr)
```

## Comparing test and train errors of the logistic model. 

```{r}
trainPred_03 = get_logistic_pred(lrgModel, diabetesTrn, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.5)

# Predict on the training data
trainErr_03 = calcErr(actual = diabetesTrn$Diabetes, predicted = trainPred_03)

# Calculate test error (already done in your code)
testErr_03 = calcErr(actual = diabetesTst$Diabetes, predicted = testPred_03)

# Compare train and test errors
errorComparison = tibble::tibble(
  Type = c("Train Error", "Test Error"),
  Error = c(trainErr_03, testErr_03)
)

knitr::kable(errorComparison)
```

## Calculating RMSE for the logistic model. 

```{r}
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}
trainLabel = as.numeric(diabetesTrn$Diabetes == "yes")
testLabel = as.numeric(diabetesTst$Diabetes == "yes")

testPredNum <- ifelse(testPred_03 == "yes", 1, 0)
trainPredNum <- ifelse(trainPred_03 == "yes", 1, 0)

trnRMSE_1 <- calc_rmse(actual = trainLabel, predicted = trainPredNum)
tstRMSE_1 <- calc_rmse(actual = testLabel, predicted = testPredNum)

rmseComparison_1 = tibble::tibble(
  Type = c("Train RMSE", "Test RMSE"),
  Error = c(trnRMSE_1, tstRMSE_1)
)

colnames(rmseComparison_1) <- c("Type", "RMSE")

knitr::kable(rmseComparison_1)

```

The error values above are too high for classifying diabetes. Sitting at a rate of 0.25, both errors carry harmful consequences, specifically the testing set. If 0.25 percent of predictions are incorrect, then misdiagnosis will occur and people won't get the treatment that they need. The RMSE values reaffirm this issue at a staggering 0.50 for each. This implies that the predicted values are, on average, 0.5 units away from the actual values. 

With high error and RMSE values, the initial model must be modified such that it reflects better accuracy. This will be done by isolating significant predictors and plugging them into a boosting model.

## Significance testing

*Create coefficient plot?*

```{r}
# Isolate the most significant predictors

modelSum <- summary(lrgModel)


knitr::kable(modelSum$coefficients)
```

We can deduce `BMI`, `GenHlth`, `Age`, `HighBP`, and `HighChol` as the most significant predictors in the initial model. As such, we will add these predictors to the initial model, `lrgModel`, along with more complexity. 


## Data prep

```{r}
# Convert training and test data to matrix format

trainMatrx = model.matrix(Diabetes ~ . + I(BMI^2) + I(GenHlth^2) + Age + HighBP + HighChol +
                          BMI:Age + HighBP:GenHlth, data = diabetesTrn)

testMatrx = model.matrix(Diabetes ~ . + I(BMI^2) + I(GenHlth^2) + Age + HighBP + HighChol +
                         BMI:Age + HighBP:GenHlth, data = diabetesTst)

```


## Using a boosting model to reduce underfitting in the model

```{r}
# Objective: sets objective function to binary classification 
# eval_metric: Specifies the evaluation metric as "error", which refers to the classification error rate
# max_depth: max depth of each decision tree, "stump".
# eta: learning rate applied to the decision tree


parameters <- list(
  objective = "binary:logistic",
  eval_metric = "error",
  max_depth = 6,
  eta = 0.1,
  nthread = 2
)

boostMod <- xgboost(
  data = trainMatrx,
  label = trainLabel,
  params = parameters,
  nrounds = 100,
  verbose = 0
)

# Predict on the test data
tstPredictions <- predict(boostMod, testMatrx)

# Apply 0.5 cutoff
tstPredLabels <- ifelse(tstPredictions > 0.5, "yes", "no")

```

## Evaluating error from boost model

```{r}
# Create predictions on the train data 
trnPredictions <- predict(boostMod, trainMatrx)

# Apply 0.5 cutoff
trnPredLabels <- ifelse(trnPredictions > 0.5, "yes", "no")

trnBoostErr = calcErr(actual = diabetesTrn$Diabetes, predicted = trnPredLabels)
tstBoostErr = calcErr(actual = diabetesTst$Diabetes, predicted = tstPredLabels)

errorComparison2 = tibble::tibble(
  Type = c("Train Error", "Test Error"),
  Error = c(trnBoostErr, tstBoostErr)
)

knitr::kable(errorComparison2)

```

## Calculate RMSE.

```{r}
# helper function for calculating RMSE

trnRMSE <- calc_rmse(actual = trainLabel, predicted = trnPredictions)
tstRMSE <- calc_rmse(actual = testLabel, predicted = tstPredictions)

rmseComparison = tibble::tibble(
  Type = c("Train RMSE", "Test RMSE"),
  Error = c(trnRMSE, tstRMSE)
)

colnames(rmseComparison) <- c("Type", "RMSE")

knitr::kable(rmseComparison)

```

## Model comparison

```{r}
# Create confusion matrix
boostTab <- table(Predicted = tstPredLabels, Actual = diabetesTst$Diabetes)
boostMatrx <- confusionMatrix(boostTab, positive = "yes")

boostMetrics <- rbind(
  c(testMatrx_03$overall["Accuracy"],
    testMatrx_03$byClass["Sensitivity"],
    testMatrx_03$byClass["Specificity"]),
  
  c(boostMatrx$overall["Accuracy"],
    boostMatrx$byClass["Sensitivity"],
    boostMatrx$byClass["Specificity"]))

rownames(boostMetrics) <- c("Standard", "Boost")

metric_comparison <- as_tibble(boostMetrics, rownames = "Model")

knitr::kable(metric_comparison)
```


**Continue with this section**

As we can see from the comparison, the boost model delivers better results compared to the previous model. 

The errors above, in comparison to the previous model, are not too different. However, the training error is slightly lower than the testing error in the boosting model, suggesting more complexity and better accuracy without over-fitting. This figures as the errors are close in value, indicating the model's ability to generalize on the test data--that is, predict well on unseen data, not just the data it was trained on. 

The RMSE values likewise reflect this generalization ability with train and test values, respectively, as 0.39 and 0.40. Not only this, but the values are an improvement from the previous RMSE values of 0.50 each, indicating a better performance on the train and test data. This improvement in accuracy is evident in the performance metrics above. 


## Evaluate 

**Hammer in the importance of RMSE and why it should be lowered. Ignore the errors. Add in ROC curve for both models?**

Two models were used to classify Diabetes, a logistic and xgboost model. For the logistic method, models with multiple cutoffs were used to identify the most accurate one, with the 0.5 cutoff yielding the best results. The model exhibited the most balanced trade off between Accuracy, Sensitivity, and Specificity, with the values, respectively, of 0.74, 0.76, 0.72. This indicated that the model was able to identify instances of diabetes with 74 percent Accuracy, with true positives and negatives sitting at 76 and 72 percent, respectively. To ensure that these metrics were not the result of under or over-fitting, the test and train error were compared. With both values sitting around 0.25, there was little reason to suspect a poor fit model, as such a case would involve a large gap between the errors. The same can be said about the train and test RMSE, which both sat around 0.5. Here, however, it was apparent that the RMSE values were too high. The predicted values were, on average, 0.5 units away from the actual values, which is significant when the range of predicted probabilities is 0 and 1. Thus, it was important to address this issue by reducing the residuals in the model.

This can be done through a gradient boost model using xgboost. In this model, "weak learners", or stumps from a decision tree, are aggregated in an ensemble model. The residuals from this model are then scaled by a learning rate and fitted to a new model. This ensures error is reduced without over-fitting. The effects of the xgboost model are evident in the RMSE values, with the train and test, respectively, sitting at 0.39 and 0.40, down from 0.50 in the logistic model. Additionally, the errors are down from 0.25, with the train and test sitting at 0.23 and 0.24, respectively. This improvement in accuracy is reaffirmed by the performance metrics, with the boost model leading in accuracy and sensitivity. 

## Conclusion

While both models accurately predicted Diabetes, the logistic model contained issues that were resolved in the boost model. With train and test RMSE values, respectively, of 0.5 each, the accuracy of the logistic model was not reliable. Thus, a boost model was used to reduce this error and improve accuracy. 
The boost model exhibited lower RMSE values, improving accuracy while maintaining the model's ability to generalize on unseen data. This is clear in the sense that the  train and test RMSE for the boost model are close together, with train RMSE slightly lower. 




