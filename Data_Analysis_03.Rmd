---
title: "Data Analysis"
author: "Luke Fisher"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::github_document
---

## Introduction

Diabetes is an chronic autoimmune disease affecting millions of Americans each year. It is best described as the body's inability to properly produce insulin, or produce any at all. This is a result of either an invalid or exhausted pancreas, whose job is to secrete enough insulin to manage blood-glucose levels. Normally, insulin is released to enable cells to absorb the blood-glucose to use for energy. In this way, it acts as a "key" between blood-glucose and cells. 

For a diabetic, however, this "key" doesn't occur naturally, instead taking the form of insulin injections. As such, a diabetic uses a glucose monitor to regulate their blood sugar--whose excess or lack thereof has detrimental consequences. For this reason, it is important to know whether or not someone is diabetic. In this project, I will use classification to identify diabetes.   

## Data Collection

The classification will be based on a Kaggle dataset derived from the CDCs Behavioral Risk Factor Surveillance System (BRFSS). The data contains 70,692 responses from the 2015 BRFSS survey, each related to risk factors like smoking, high cholesterol, and physical activity. Furthermore, the data contains an equal 50-50 split of respondents with and without diabetes. 

The data is binary, meaning that the predictors take on a value one or zero depending on whether the condition is true or not. For instance, if a respondent has a smoking habit they will be marked with a 1 for the smoking column; otherwise, they will receive a 0. There are some exceptions to this like BMI and age, where the values are continuous. 

## Methodology

The classification will be done by binary logistic regression. As such, the response variable, diabetes, will take on two values, "yes" or "no", corresponding to whether the patient has the disease. The classification will start with a series of logistic models, each with a unique cutoff. The function will label, respectively, "yes" and "no" for the values above and below 0.5. Afterwards, these predicted values will be compared with the actual values in a table and put into a confusion matrix for evaluation. The method above will repeat itself with one large model with multiple cutoffs. The point of this is to ensure consistent results. 

```{r setup, echo=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(dplyr)
library(ggplot2)
library(ISLR)
library(tibble)
library(caret)
library(tidyr)
library(skimr)
library(glmnet)
library(car)
library(xgboost)
```

## Load in data 
```{r}
diabetesData <- read.csv('/Users/lukefisher/Desktop/Coding/repos/Health_Analytics/Data/Diabetes_Indicators_Binary.csv')
```

## Data Wrangling
```{r}
diabetesData <- diabetesData %>% 
rename(Diabetes = Diabetes_binary) %>%
mutate(Diabetes = factor(Diabetes, levels = c(0, 1), labels = c("no", "yes")))

str(diabetesData)
```

## Cross validation
```{r}
# Split the data into an 80/20 train vs. test split. Set the seed for replicability.
set.seed(44222)

diabetesIdx = sample((nrow(diabetesData)), size = 0.8 * nrow(diabetesData))
diabetesTrn = diabetesData[diabetesIdx, ]
diabetesTst = diabetesData[-diabetesIdx, ]

head(diabetesTrn, n = 10)
```

```{r}
get_logistic_pred = function(mod, data, res = "y", pos = 1, neg = 0, cut = 0.5) {
  probs = predict(mod, newdata = data, type = "response")
  ifelse(probs > cut, pos, neg)
}


# Creating separate predictions based on different cutoffs

lrgModel = glm(Diabetes ~ . -1, data = diabetesTrn, family = "binomial")

testPred_01 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.1)

testPred_02 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.33)

testPred_03 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.5)

testPred_04 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.66)

testPred_05 = get_logistic_pred(lrgModel, diabetesTst, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.9)


# Evaluate Accuaracy, Sensitivity, and Specificity for each cutoff
testTab_01 <- table(predicted = testPred_01, actual = diabetesTst$Diabetes)
testTab_02 <- table(predicted = testPred_02, actual = diabetesTst$Diabetes)
testTab_03 <- table(predicted = testPred_03, actual = diabetesTst$Diabetes)
testTab_04 <- table(predicted = testPred_04, actual = diabetesTst$Diabetes)
testTab_05 <- table(predicted = testPred_05, actual = diabetesTst$Diabetes)


testMatrx_01 <- confusionMatrix(testTab_01, positive = "yes")
testMatrx_02 <- confusionMatrix(testTab_02, positive = "yes")
testMatrx_03 <- confusionMatrix(testTab_03, positive = "yes")
testMatrx_04 <- confusionMatrix(testTab_04, positive = "yes")
testMatrx_05 <- confusionMatrix(testTab_05, positive = "yes")


metrics <- rbind(
  c(testMatrx_01$overall["Accuracy"],
    testMatrx_01$byClass["Sensitivity"],
    testMatrx_01$byClass["Specificity"]),

  c(testMatrx_02$overall["Accuracy"],
    testMatrx_02$byClass["Sensitivity"],
    testMatrx_02$byClass["Specificity"]),

  c(testMatrx_03$overall["Accuracy"],
    testMatrx_03$byClass["Sensitivity"],
    testMatrx_03$byClass["Specificity"]),
    
  c(testMatrx_04$overall["Accuracy"],
    testMatrx_04$byClass["Sensitivity"],
    testMatrx_04$byClass["Specificity"]),

  c(testMatrx_05$overall["Accuracy"],
    testMatrx_05$byClass["Sensitivity"],
    testMatrx_05$byClass["Specificity"])
)

rownames(metrics) = c("c = 0.10", "c = 0.33", "c = 0.50", "c = 0.66", "c = 0.90")

metrics_tibble <- as_tibble(metrics, rownames = "Threshold")

print(knitr::kable(metrics_tibble))
```

The table above contains regression models with varying cutoffs. The model with a 0.5 cutoff appears to have the most balanced trade-off between Accuracy, Specificity, and Sensitivity, exhibiting characteristics of a valid classifier. 

## Test errors

```{r}
# Test for error in the 0.5 prediction
calcErr = function(actual, predicted) {
  mean(actual != predicted)
}

calcErr(actual = diabetesTst$Diabetes, predicted = testPred_03)
```

## Comparing test and train data to test over-fitting.

```{r}
trainPred_03 = get_logistic_pred(lrgModel, diabetesTrn, res = "Diabetes", 
pos = "yes", neg = "no", cut = 0.5)

# Predict on the training data
trainErr_03 = calcErr(actual = diabetesTrn$Diabetes, predicted = trainPred_03)

# Calculate test error (already done in your code)
testErr_03 = calcErr(actual = diabetesTst$Diabetes, predicted = testPred_03)

# Compare train and test errors
errorComparison = tibble::tibble(
  Type = c("Train Error", "Test Error"),
  Error = c(trainErr_03, testErr_03)
)

knitr::kable(errorComparison)

```

Since the train and test errors are closely aligned, there is an indication of an under-fit model. This implies that the model is too simple and cannot capture underlying patterns in the data. 

## Significance testing

```{r}
# Isolate the most significant predictors

modelSum <- summary(lrgModel)


print(modelSum$coefficients)
```

We can deduce BMI, GenHlth, Age, HighBP, and HighChol as the most significant predictors in the initial model. As such, we will use these for the following boosting model. 


## Data prep

```{r}
# Convert training and test data to matrix format

trainMatrx = model.matrix(Diabetes ~ BMI^4 + GenHlth^2 + Age + HighBP + HighChol
                          -1, data = diabetesTrn)
testMatrx = model.matrix(Diabetes ~ BMI^4 + GenHlth^2 + Age + HighBP + HighChol
                         -1, data = diabetesTst)

# Convert response variable to binary 

trainLabel = as.numeric(diabetesTrn$Diabetes == "yes")
testLabel = as.numeric(diabetesTst$Diabetes == "yes")

```


## Using a boosting model to reduce underfitting in the model

```{r}
parameters <- list(
  objective = "binary:logistic",
  eval_metric = "error",
  max_depth = 6,
  eta = 0.3,
  nthread = 2
)

boostMod <- xgboost(
  data = trainMatrx,
  label = trainLabel,
  params = parameters,
  nrounds = 100,
  verbose = 0
)

# Predict on the test data
predictions <- predict(boostMod, testMatrx)

# Apply 0.5 cutoff
predLabels <- ifelse(predictions > 0.5, "yes", "no")

# Create confusion matrix
boostTab <- table(Predicted = predLabels, Actual = diabetesTst$Diabetes)
boostMatrx <- confusionMatrix(boostTab, positive = "yes")

# print(boostMatrx)
calcErr(actual = diabetesTst$Diabetes, predicted = testPred_03)
calcErr(actual = diabetesTst$Diabetes, predicted = predLabels)

```




The plan above is to isolate the most significant predictors in the initial model by measuring their p-values. The predictors with the lowest p-values (i.e., p-value <0.05) are added to matrices for the boosting model. This ensures that the most significant predictors are used, and the test error in the boosting model is lowered from its initial value. 



## Evaluate 


The above model exhibits different levels of Accuracy, Sensitivity, and Specificity at different cutoffs. This implies a change the amount of positive and negative cases captured, (i.e., 1 for positive, 0 for negative) meaning that the values for Accuracy, Specificity, and Sensitivity are a direct reflection of however many positive and negative cases there are. For example, it is no surprise that the first model captures 99 percent of true positives under a 0.10 cutoff. It practically only captures positive cases. The inverse is true for the last model. 

With that said, the model with the most balanced trade-off between Accuracy, Sensitivity, and Specificity is the model with a 0.5 cutoff. It differs from the other models in the sense that it doesn't skew toward one metric, making for a unbiased classifier. Furthermore, the ROC curve hugs the top-left around the 0.50 mark, where the model exhibits its highest true positive and negative rates. The overall performance of the model is 0.82, meaning that it has a solid ability to discriminate between diabetics and non-diabetics. 

## Conclusion














